{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Study I\n",
    "\n",
    "import gym\n",
    "from EduSim.Envs.TMS import TMSEnv, tms_train_eval\n",
    "from EduSim import MetaAgent\n",
    "\n",
    "env: TMSEnv = gym.make(\n",
    "    \"TMS-v1\",\n",
    "    name=\"binary\",\n",
    "    mode=\"no_measurement_error\",\n",
    "    seed=10,\n",
    ")\n",
    "\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "import numpy as np\n",
    "from EduRec.RLA.DQN import DQNet, get_loss, fit_f, net_init\n",
    "from EduRec.meta.ReplayBuffer import CircularReplayBuffer as ReplayBuffer\n",
    "from longling.ML.MxnetHelper import get_trainer\n",
    "from longling.ML.MxnetHelper.utils import Configuration\n",
    "\n",
    "class Agent(MetaAgent):\n",
    "    def __init__(self, action_space, seed=None):\n",
    "        super(Agent, self).__init__()\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.observation = None\n",
    "        self.action_idx = None\n",
    "        self.reward = None\n",
    "\n",
    "        self.value_net_cfg = Configuration()\n",
    "        self.value_net_cfg.optimizer_params[\"learning_rate\"] = 0.01\n",
    "        self.value_net = DQNet(self.action_space.shape[0])\n",
    "        net_init(self.value_net, self.value_net_cfg)\n",
    "        self.value_net_tune_freq = 64\n",
    "        self.value_net_tune_cnt = 0\n",
    "        self.value_net_replay_buffer = ReplayBuffer(seed=seed)\n",
    "        self.value_net_cfg.batch_size = 64\n",
    "        self.value_net_loss = get_loss()\n",
    "        self.value_net_trainer = get_trainer(\n",
    "            self.value_net, self.value_net_cfg.optimizer, self.value_net_cfg.optimizer_params,\n",
    "            self.value_net_cfg.lr_params\n",
    "        )\n",
    "        self._random_state = np.random.RandomState(seed)\n",
    "\n",
    "    def begin_episode(self, learner_profile, *args, **kwargs):\n",
    "        _, observation = learner_profile\n",
    "        self.observation = observation\n",
    "\n",
    "    def end_episode(self, observation, reward, done, info):\n",
    "        pass\n",
    "\n",
    "    def observe(self, observation, reward, done, info):\n",
    "        self.reward = reward\n",
    "        self.value_net_replay_buffer.add(\n",
    "            [self.observation, self.action_idx, self.reward]\n",
    "        )\n",
    "        self.value_net_tune_cnt += 1\n",
    "        if self.value_net_tune_cnt >= self.value_net_tune_freq:\n",
    "            self.tune()\n",
    "            self.value_net_tune_cnt -= self.value_net_tune_freq\n",
    "        self.observation = observation\n",
    "\n",
    "    def step(self):\n",
    "        action_idx = int(self.value_net(nd.array([self.observation], self.value_net_cfg.ctx)).argmax().asscalar())\n",
    "        self.action_idx = action_idx\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def tune(self, *args, **kwargs):\n",
    "        observation, action, reward = zip(*self.value_net_replay_buffer.sample(self.value_net_cfg.batch_size))\n",
    "        observation = nd.array(observation, dtype=\"float32\")\n",
    "        action = nd.array(action, dtype=\"int\")\n",
    "        reward = nd.array(reward, dtype=\"float32\")\n",
    "        fit_f(\n",
    "            self.value_net, (observation, action, reward), self.value_net_loss, self.value_net_trainer,\n",
    "            batch_size=self.value_net_cfg.batch_size,\n",
    "            ctx=self.value_net_cfg.ctx\n",
    "        )\n",
    "\n",
    "agent = Agent(env.action_space, seed=10)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from longling.ML.toolkit.monitor import MovingLoss\n",
    "from EduSim.utils import ConsoleProgressMonitor, EMAValue\n",
    "\n",
    "values = {\n",
    "    \"Episode\": EMAValue([\"Reward\"])\n",
    "}\n",
    "loss_monitor = MovingLoss(agent.value_net_loss)\n",
    "values.update({\"Net\": loss_monitor.losses})\n",
    "\n",
    "indexes = {\"Episode\": [\"Reward\"], \"Net\": list(agent.value_net_loss)}\n",
    "max_episode_num = 100 * 100  # 4.1.2\n",
    "\n",
    "monitor = ConsoleProgressMonitor(\n",
    "    indexes=indexes,\n",
    "    values=values,\n",
    "    total=max_episode_num,\n",
    "    player_type=\"episode\"\n",
    ")\n",
    "\n",
    "from longling import set_logging_info\n",
    "set_logging_info()\n",
    "tms_train_eval(\n",
    "    agent,\n",
    "    env,\n",
    "    max_steps=2,\n",
    "    max_episode_num=max_episode_num,  # 4.1.2\n",
    "    level=\"summary\",\n",
    "    board_dir=\"./tms_binary\",\n",
    "    monitor=monitor,\n",
    "    values=values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}